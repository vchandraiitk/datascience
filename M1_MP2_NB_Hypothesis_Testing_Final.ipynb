{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vchandraiitk/datascience/blob/main/M1_MP2_NB_Hypothesis_Testing_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUUu9l_JfJ92"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "\n",
        "##  A program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook 2: Hypothesis Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL3yrUc-XrLS"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq2_Otle4nO2"
      },
      "source": [
        "\n",
        "\n",
        "At the end of this Mini Project, you will be able to :\n",
        "\n",
        "\n",
        "* Have a very surface and high-level level understanding of A/B Testing - a statistical way to compare two or more versions (A or B?)\n",
        "\n",
        "* Determine not only which one (A or B) performs better but also to understand if the difference between two of them is statistically significant\n",
        "\n",
        "* Derive meaningful insights from the formula for Confidence Intervals with the t-distribution and how to adjust this formula using the Central Limit Theorem\n",
        "\n",
        "* Know the dependant 3 factors (Power of the test, Significance level & Minimal Desired Effect) for having a required Sample Size\n",
        "\n",
        "* Carry out Binomial Proportion Confidence Intervals, 2-sample Z-test, Hypothesis testing on difference & Effect Size & Chi-square test. The objective of this mini project is to serve as an introductory guide to A/B testing, covering foundational concepts and methodologies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Ff_XtzI0f1"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M19d-lHWImDi"
      },
      "source": [
        "A/B tests are very commonly performed by data analysts and data scientists. It is important to get some practice working with these difficulties.\n",
        "\n",
        "A/B testing is a crucial technique in data-driven decision-making, yet it often lacks comprehensive exploration. This notebook aims to address this gap by providing a consolidated overview of A/B testing principles and practices.\n",
        "\n",
        "For this mini project, you will be working to understand the results of an A/B test run by an e-commerce website. Your goal is to work through this notebook to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EqvTSjZZIUE"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlCSHY5_Y0wb"
      },
      "source": [
        "The dataset chosen for this experiment is the **ab_data.csv** dataset which is publicly available on [Kaggle](https://www.kaggle.com/datasets/abdelrahmanrezk7/ab-testing-e-commerce-website)  \n",
        "\n",
        "This dataset consists of 2,94,478 records. Each record is made up of 5 fields.\n",
        "\n",
        "**For example**, Each record consists of 'user_id', 'timestamp', 'group', 'landing_page' and 'converted'.\n",
        "\n",
        "* **user_id:** A unique identifier assigned to each user participating in the experiment.\n",
        "\n",
        "* **timestamp:** The timestamp indicating the time at which the user interacted with the webpage or was exposed to the experimental condition.\n",
        "\n",
        "* **group:** The group to which the user was assigned, typically denoted as either 'control' or 'treatment'. This field helps categorize users into different experimental conditions.\n",
        "\n",
        "* **landing_page:** Specifies the type of landing page or webpage variant that the user was directed to upon interaction. It distinguishes between different versions of the webpage used in the experiment.\n",
        "\n",
        "* **converted:** A binary indicator representing whether the user performed the desired action or conversion after interacting with the webpage. It typically indicates whether the user made a purchase, signed up for a service, or completed any other desired action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3vgcWwOF2cK"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYdLgvhhZtwA"
      },
      "source": [
        "The biggest e-commerce company called FaceZonGoogAppFlix approached to a data science consulting firm as a new client!\n",
        "\n",
        "They have a potential new webpage designed with the intention to increase their current conversion rates of 12% by 0.35% or more. With such an ambiguous task, they have full trust in the data science consulting firm to give them a recommendation whether to implement the new pagewebpage or keep the old webpage. Unforunately they haven't built up a data science capability in their company, but they've used an external software called 'A/B Tester' for 23 days and then come back to the data science consulting firm with a dataset. Under this requirement scenario, what the data science consulting firm will do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQrRWcHcSFrY"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwmmvQnv3mLM"
      },
      "outputs": [],
      "source": [
        "# @title Download the Dataset\n",
        "! wget -q https://cdn.exec.talentsprint.com/static/cds/content/ab_data.csv\n",
        "print(\"The datset was downloaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkN12h1bjv2z"
      },
      "source": [
        "# **Part I - Probability**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epVoy2b_Z05e"
      },
      "source": [
        "#### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBpCF4GlBPFL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as ss\n",
        "import statsmodels.api as sm\n",
        "import math as mt\n",
        "import itertools\n",
        "import random\n",
        "from patsy import dmatrices\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#We are setting the seed to assure that each of your mini project peer group members gets the same answers\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmOJDVdp9PYo"
      },
      "source": [
        "#### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DrVCIg54LZp"
      },
      "outputs": [],
      "source": [
        "# a. Read in the dataset and take a look at the top few rows here:\n",
        "# YOUR CODE HERE\n",
        "df = pd.read_csv('https://cdn.exec.talentsprint.com/static/cds/content/ab_data.csv')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9Jg6Z0KML6r"
      },
      "outputs": [],
      "source": [
        "# b. Use the below cell to find the number of rows in the dataset.\n",
        "# YOUR CODE HERE\n",
        "df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li5KS0i3pQqq"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edt4IHsO4lua"
      },
      "source": [
        "### Task 1: Data Cleaning\n",
        "\n",
        "* Check the number of unique users in the dataset\n",
        "\n",
        "* Check the proportion of users converted.\n",
        "  \n",
        "    **Hint:** query(), count()\n",
        "* Estimate how many times the new_page and treatment don't line up. Also estimate how many times the old_page and control do not match.\n",
        "\n",
        "* Display the total no. of non-line up pages\n",
        "\n",
        "* Check if any of the rows have missing values?\n",
        "\n",
        "#### **Treatment Group & Control Group**\n",
        "* **Treatment Group (New Webpage):**\n",
        "Users in this group will be exposed to the new webpage design.\n",
        "The effectiveness of the new webpage design will be measured by comparing the conversion rates of users (who actually make purchase of the company's products after visiting this new webpage) in this group to those in the control groups.\n",
        "* **Control Group 1 (Placebo):**\n",
        "Users in this group will be presented with an identical-looking webpage that serves as a placebo.\n",
        "This group represents the baseline scenario where users are exposed to the current webpage design without any changes.\n",
        "It means that in Control Group 1, users will see a webpage that looks exactly like the current one (new one) but doesn't have any actual changes. This group helps us understand how users typically behave on the current webpage without any alterations. It's like giving users a fake version of the webpage to see how they respond, so we can compare their behavior to those users who see the real changes in the actual new webpage.\n",
        "* **Control Group 2 (Old Webpage or Existing Treatment):**\n",
        "Users in this group will be shown a webpage that is already in use and has demonstrated effectiveness in terms of conversion rates. It means that users in Control Group 2 will see the same old webpage that is currently being used. This webpage has been proven to be effective in terms of converting visitors into purchasers (or customers) in the past.\n",
        "\n",
        "-- This group serves as a benchmark to evaluate whether the new webpage design outperforms the existing treatment.\n",
        "\n",
        "-- This group (Control Group 2) acts as a standard for comparison to see if the new webpage design performs better than the current one. We will use the conversion rates observed in Control Group 2 to assess whether the changes made in the new webpage design lead to better results or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGXvWzHWZTUg"
      },
      "outputs": [],
      "source": [
        "# c. The number of unique users in the dataset.\n",
        "# YOUR CODE HERE\n",
        "df.user_id.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9Vj8ld6ZaIg"
      },
      "outputs": [],
      "source": [
        "#Check the proportion of users converted.\n",
        "df.query('converted == 1')['converted'].count() / df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-3Wh2RL4Z0t"
      },
      "outputs": [],
      "source": [
        "# identify treatment does not match with new_page\n",
        "# YOUR CODE HERE\n",
        "df_treatment = df.query(\"group=='treatment' and landing_page!='new_page'\")\n",
        "df_treatment.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIFju1AqFd1t"
      },
      "outputs": [],
      "source": [
        "# identify control does not match with old_page\n",
        "# YOUR CODE HERE\n",
        "df_control = df.query(\"group=='control' and landing_page!='old_page'\")\n",
        "df_control.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v15b7XvZPSF6"
      },
      "outputs": [],
      "source": [
        "# Total no. of non-line up\n",
        "# YOUR CODE HERE\n",
        "#df_treatment.shape[0]+df_control.shape[0]\n",
        "#df.query(\"group=='control' and landing_page!='old_page'\").add(df.query(\"group=='treatment'\").query(\"landing_page!='new_page'\")).shape[0]\n",
        "df.query(\"(group=='control' and landing_page!='old_page') or (group=='treatment' and landing_page!='new_page')\").shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnAgcxCHGywv"
      },
      "outputs": [],
      "source": [
        "# Check for any missing values\n",
        "# YOUR CODE HERE\n",
        "#user_id\ttimestamp\tgroup\tlanding_page\tconverted\n",
        "#df.isna().any()\n",
        "df.isna().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqq4CX4gG3zU"
      },
      "outputs": [],
      "source": [
        "# Check datatype of each column\n",
        "# YOUR CODE HERE\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PVM_RBKbjZ6"
      },
      "source": [
        "### Task 2: Identify the not aligned rows\n",
        "\n",
        "<u>**Part-2a:**</u>\n",
        "\n",
        "With the above dataset (achieved in Task-1) the requirement is to first identify the rows in that dataset where the treatment group is aligned with the new_page and where the control group is aligned with the old_page.\n",
        "\n",
        "**Hint:** It creates a new DataFrame containing these filtered rows.\n",
        "**('group == \"treatment\" and landing_page == \"new_page\"')**\n",
        "\n",
        "<u>**Part-2b:**</u>\n",
        "\n",
        "Now, with the help of the new dataset (achieved in Task-2), we need to identify the misaligned rows in the dataset (achieved in Task-1) where treatment is not aligned with new_page or control is not aligned with old_page\n",
        "\n",
        "This can be done by checking the values 'treatment' and 'control' under the 'group' column to ensure they do not correspond with the values 'new_page' and 'old_page' under the 'landing_page' column, respectively.\n",
        "\n",
        "For the rows where treatment is not aligned with new_page or control is not aligned with old_page, we cannot be sure if this row truly received the new or old page. Write your code to provide how we should handle these rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibXbIHHPQXZL"
      },
      "outputs": [],
      "source": [
        "# Part-2a\n",
        "# create a new dataset that meets the specifications:\n",
        "# treatment is aligned with new_page or control is aligned with old_page\n",
        "#df.query(\"group=='treatment' and landing_page=='new_page'\").shape\n",
        "# YOUR CODE HERE\n",
        "#df_filtered = df.query(\"group=='control' and landing_page!='old_page' or group=='treatment' and landing_page!='new_page'\")\n",
        "df_filtered = df.query(\"(group=='control' and landing_page=='old_page') or (group=='treatment' and landing_page=='new_page')\")\n",
        "df_filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpE9zOpnTn8n"
      },
      "outputs": [],
      "source": [
        "# show the dimension of the new dataset created\n",
        "# YOUR CODE HERE\n",
        "df_filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovn38S2RrSH7"
      },
      "outputs": [],
      "source": [
        "# Part-2b\n",
        "# Now, with the help of the new dataset (achieved above), identify the misaligned rows in the dataset (achieved in Task-1)\n",
        "# where treatment is not aligned with new_page or control is not aligned with old_page\n",
        "# YOUR CODE HERE\n",
        "merged_df = df.merge(df_filtered, on=list(df.columns), how='left', indicator=True)\n",
        "merged_df\n",
        "result_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "result_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "misaligned_df = df[~df.index.isin(df_filtered.index)]\n",
        "misaligned_df.shape"
      ],
      "metadata": {
        "id": "ksqym15F3oPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJyZYPBlTlIl"
      },
      "outputs": [],
      "source": [
        "# Double Check all of the correct rows were removed - this should be 0\n",
        "# YOUR CODE HERE\n",
        "df_filtered.query(\"group=='control' and landing_page!='old_page' or group=='treatment' and landing_page!='new_page'\").shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc3UOT74MM3O"
      },
      "source": [
        "### Task-3:Using the above new dataset, answer the following questions.\n",
        "\n",
        "* How many unique user_ids are in the new dataset created above in Task-2?\n",
        "\n",
        "* There is one user_id repeated in this dataset. What is it? (Here you need to show only the user_id)\n",
        "\n",
        "* What is the row information for the repeat user_id? (Here, you need to show the complete row including 'user_id', 'timestamp', 'group', 'landing_page' and\t'converted')\n",
        "\n",
        "* Remove one of the rows with a duplicate user_id, but keep your dataframe name as same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dn_ljWWAcNM"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "df_filtered['user_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the duplicated user_id\n",
        "# drop the duplicate user_id, keep your dataframe name as same.\n",
        "duplicate_user_id=df_filtered[df_filtered.duplicated('user_id', keep=False)]['user_id'].unique()\n",
        "print(duplicate_user_id)"
      ],
      "metadata": {
        "id": "ktcX1T4QnFpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7MFS8RWbW7N"
      },
      "outputs": [],
      "source": [
        "# show the row information of the duplicate user_id\n",
        "\n",
        "df_filtered[df_filtered['user_id'].isin(duplicate_user_id)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the duplicate user_id, keep your dataframe name as same.\n",
        "df_filtered=df_filtered.drop_duplicates(subset='user_id')"
      ],
      "metadata": {
        "id": "Ioj3Z01M3_UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW-LBmqtSMon"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7NPwyNyUfGO"
      },
      "outputs": [],
      "source": [
        "df_filtered.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrPzlHc4-zIR"
      },
      "source": [
        "## Finding Probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFgtC_jCpJL1"
      },
      "source": [
        "### Task 4: After removing the duplicated user_id, answer the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QQ2WUQX9XYy"
      },
      "source": [
        "##### Exercise 1: What is the probability of an individual converting regardless of the page they receive?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcRjzr9YRo72"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "#df_filtered['converted'].sum()/df_filtered.shape[0]\n",
        "round(df_filtered['converted'].sum()/df_filtered.shape[0], 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzzEnMv25vGn"
      },
      "source": [
        "##### Exercise 2: Given that an individual was in the control group, what is the probability they converted?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWHrrw2BJrb7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "#df_filtered.query('group == \"control\"').shape[0]/df_filtered.shape[0]\n",
        "#df_filtered.query('group == \"control\" and \"converted\"==1')/df_filtered.query('group == \"control\"')\n",
        "control_convert = df_filtered.query('group == \"control\" and converted == 1').shape[0]/df_filtered.query('group == \"control\"').shape[0]\n",
        "#control_convert\n",
        "probability_converted_control=round(control_convert, 4)\n",
        "probability_converted_control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFNIQ4dj59Ep"
      },
      "source": [
        "##### Exercise 3: Given that an individual was in the treatment group, what is the probability they converted?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZDWxb_JRtBl"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "treatment_convert = df_filtered.query('group == \"treatment\" and converted == 1').shape[0]/df_filtered.query('group == \"treatment\"').shape[0]\n",
        "#treatment_convert\n",
        "probability_converted_treatment=round(treatment_convert, 4)\n",
        "probability_converted_treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBreBsQ7Vqi0"
      },
      "source": [
        "\n",
        "##### Exercise 4: What is the probability that an individual received the new page?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFUmNVsCTR_t"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "#df_filtered.query('group == \"control\"').shape[0]/df_filtered.shape[0]\n",
        "round(df_filtered.query('landing_page==\"new_page\"').shape[0]/df_filtered.shape[0], 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDMFaGnT8NsJ"
      },
      "outputs": [],
      "source": [
        "# What is the probability that an individual received the old page?\n",
        "# YOUR CODE HERE\n",
        "#df_filtered.query(\"landing_page!='old_page'\").shape[0]/df_filtered.shape[0]\n",
        "round(df_filtered.query(\"landing_page=='old_page'\").shape[0]/df_filtered.shape[0], 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YKbXa-R8dCO"
      },
      "outputs": [],
      "source": [
        "# show the difference: (treatment_convert - control_convert)\n",
        "# YOUR CODE HERE\n",
        "#treatment_convert - control_convert\n",
        "diff = df_filtered.query('group == \"treatment\" and converted == 1').shape[0]-df_filtered.query('group == \"control\" and converted == 1').shape[0]\n",
        "print(\"difference in df:\",diff)\n",
        "##difference in probability\n",
        "diff=probability_converted_treatment - probability_converted_control\n",
        "print(\"difference in probability df:\",diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiOWt855DsZ8"
      },
      "source": [
        "##### Exercise 5: Use the results in the previous two portions of this question to suggest if you think there is evidence that one page leads to more conversions? Write your response below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr5JV56sf5ra"
      },
      "source": [
        "#### **Write your response here:** This difference indicates that the conversion rate in the treatment group is slightly lower than in the control group by 0.15 percentage\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpqFMlUxkuMR"
      },
      "source": [
        "# **Part II - A/B Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7IFjnNlDVn"
      },
      "source": [
        "Notice that because of the time stamp associated with each event, you could technically run a hypothesis test continuously as each observation was observed.\n",
        "\n",
        "However, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time? How long do you run to render a decision that neither page is better than another?\n",
        "\n",
        "These questions are the difficult parts associated with A/B tests in general.\n",
        "\n",
        "1. For now, consider you need to make the decision just based on all the data provided. If you want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5% (i.e. 0.05), what should your null and alternative hypotheses be? You can state your hypothesis in terms of words or in terms of $p_{old}$ and $p_{new}$, which are the converted rates for the old and new pages.\n",
        "\n",
        "$$  H_{null} : p_{new} - p_{old} <= 0 $$  \n",
        "\n",
        "$$  H_{alternative} : p_{new} - p_{old} > 0 $$\n",
        "\n",
        "2. Assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the converted success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assume they are equal to the converted rate in ab_data.csv regardless of the page.\n",
        "\n",
        "Use a sample size for each page equal to the ones in ab_data.csv.\n",
        "\n",
        "Perform the sampling distribution for the difference in converted between the two pages over 10,000 iterations of calculating an estimate from the null."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEG31avWsX2x"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "#overall_conversion = df_filtered.query('converted == 1').shape[0]/df_filtered.shape[0]\n",
        "sampling_array=[]\n",
        "treatment_size = df_filtered.query('group == \"treatment\"').shape[0]\n",
        "control_size = df_filtered.query('group == \"control\"').shape[0]\n",
        "for i in range (0, 10000):\n",
        "    treatment = np.random.choice([0,1], treatment_size).sum()/treatment_size\n",
        "    control = np.random.choice([0,1], control_size).sum()/control_size\n",
        "    sampling_array.append(treatment-control)\n",
        "sampling_df = pd.DataFrame(sampling_array)\n",
        "sampling_df.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff_amVABXbL3"
      },
      "outputs": [],
      "source": [
        "df_filtered.sample(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boBhK2SdGXlW"
      },
      "source": [
        "### Task 5: Derive the below insights\n",
        "\n",
        "* **$n_{old}$** - The number of individuals (each having unique user_id) who received the old page (control group)\n",
        "\n",
        "* **$n_{new}$** - The Number of individuals (each having unique user_id) who received the new page (treatment group)\n",
        "\n",
        "* **$p_{new}$ under the null:** This refers to the conversion rate for the new page (treatment group) that we would expect to observe if the null hypothesis ($H_{null}$) is true.\n",
        "In other words, it represents the probability of conversion for users who are exposed to the new page under the assumption that there is no difference in conversion rates between the old and new pages.\n",
        "\n",
        "So, basically **$p_{new}$ under the null** is the conversion rate for the new page that we would expect to see if the new page had no impact on user behavior compared to the old page. It serves as a reference point for evaluating the observed conversion rate for the new page in relation to the null hypothesis ($H_{null}$).\n",
        "\n",
        "* **$p_{old}$ under the null:** This refers to the conversion rate for the old page (control group) that we would expect to observe if the null hypothesis ($H_{null}$) is true.\n",
        "In other words, it represents the probability of conversion for users who are exposed to the old page under the assumption that there is no difference in conversion rates between the old and new pages.\n",
        "\n",
        "So, basically **$p_{old}$ under the null** is the conversion rate for the old page that we would expect to see if the old page had no impact on user behavior compared to the new page. It serves as a reference point for evaluating the observed conversion rate for the old page in relation to the null hypothesis ($H_{null}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtVLkGB_ANwf"
      },
      "source": [
        "##### Exercise 1: What is the convert rate for $p_{new}$ under the null?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vZnqsXZxzdG"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "p_new = df_filtered.query(\"landing_page == 'new_page' and converted == 1\")['converted'].value_counts()/df_filtered.query(\"landing_page == 'new_page'\").shape[0]\n",
        "p_old = df_filtered.query(\"landing_page == 'old_page' and converted == 1\")['converted'].value_counts()/df_filtered.query(\"landing_page == 'old_page'\").shape[0]\n",
        "#print(p_new)\n",
        "#print(p_old)\n",
        "pdiff = p_new-p_old\n",
        "print(pdiff)\n",
        "#df_filtered.query('converted==1').shape[0]/df_filtered.shape[0]\n",
        "#ùëù under the null\n",
        "p = df_filtered.query('converted==1').shape[0]/df_filtered.shape[0]\n",
        "#Under the null ùëùùëõùëíùë§ = ùëù\n",
        "p_new = p\n",
        "##------Final Answer\n",
        "round(p_new, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb861WejndKI"
      },
      "source": [
        "##### Exercise 2: What is the convert rate for $p_{old}$ under the null?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWftl4eC2jNU"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "#Under the null ùëùùëúùëôùëë = ùëù\n",
        "p_old = p\n",
        "round(p_old, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kksy2sD4CMKQ"
      },
      "source": [
        "##### Exercise 3: What is $n_{new}$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZpnlYfHCO3P"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "n_new = df_filtered.query('group == \"treatment\" and landing_page == \"new_page\"').shape[0]\n",
        "n_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnhRhfWadnGK"
      },
      "source": [
        "##### Exercise 4: What is $n_{old}$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACEuvXBApg1y"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "n_old = df_filtered.query('group == \"control\" and landing_page == \"old_page\"').shape[0]\n",
        "n_old"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpJ67AZKuJl3"
      },
      "source": [
        "this one was tricky. But here we are looking at a null where there is no difference in conversion based on the page, which means the conversions for each page are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSCEb0GX5-d1"
      },
      "source": [
        "##### Exercise 5: Simulate $n_{new}$ transactions with a convert rate of $p_{new}$ under the null. Store these $n_{new}$ 1's and 0's in **new_page_converted**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8kGpmMmx7HI"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "new_page_converted = []\n",
        "\n",
        "for i in range (0, n_new):\n",
        "    num = np.random.random()\n",
        "    if num < p_new:\n",
        "        new_page_converted.append(0)\n",
        "    else:\n",
        "        new_page_converted.append(1)\n",
        "#sum(new_page_converted)/len(new_page_converted)\n",
        "#new_page_converted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXQfqIixzC4_"
      },
      "source": [
        "##### Exercise 6: Simulate $n_{old}$ transactions with a convert rate of $p_{old}$ under the null. Store these $n_{old}$ 1's and 0's in **old_page_converted**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB7M6Sx74bFU"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "old_page_converted = []\n",
        "for i in range (0, n_old):\n",
        "    num = np.random.random()\n",
        "    if num < p_old:\n",
        "        old_page_converted.append(0)\n",
        "    else:\n",
        "        old_page_converted.append(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGMPVx-oLVDM"
      },
      "source": [
        "##### Exercise 7: Find $(p_{new} - p_{old})$ for your simulated values from Exercise 5 and Exercise 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCqAHD0OLyFj"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "diff = sum(new_page_converted)/len(new_page_converted) - sum(old_page_converted)/len(old_page_converted)\n",
        "round(diff, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTwkV1aqJ2VL"
      },
      "source": [
        "##### Exercise 8: Simulate 10,000 $(p_{new}-p_{old})$ values using this same process similarly to the one you calculated in Exercise 1 through Exercise 7 above. Store all 10,000 values in $p_{diffs}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocQ4FNDHxmKg"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "p_diffs=[]\n",
        "treatment_len = df_filtered.query('group == \"treatment\"').shape[0]\n",
        "control_len = df_filtered.query('group == \"control\"').shape[0]\n",
        "for i in range (0, 10000):\n",
        "    p_sim_new = np.random.choice([0,1], treatment_len).sum()/treatment_len\n",
        "    p_sim_old = np.random.choice([0,1], control_len).sum()/control_len\n",
        "    p_diffs.append(p_sim_new-p_sim_old)\n",
        "#p_diffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmGeHlhncksr"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(p_diffs).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGLfvkPOU0V"
      },
      "source": [
        "Alternatively, we can do the same approach using the following snippet in order to eliminate the use of for:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb8t1F_DvgpD"
      },
      "outputs": [],
      "source": [
        "new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new\n",
        "old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old\n",
        "l = new_converted_simulation - old_converted_simulation\n",
        "pd.DataFrame(l).plot()\n",
        "# Essentially, we are applying the null proportion to the total size of each page using the binomial distribution.\n",
        "# Each element, for example, innp.random.binomial(n_new, p_new, 10000) results in an array with values like [17262, 17250, 17277...].\n",
        "# This array is 10000 elements large When we divide it by n_new, Python broadcasts n_new for each element and we return a proportion for each element.\n",
        "# This is essentially is simulating, 10000, the new page conversion rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzh2XHhV2Rrr"
      },
      "source": [
        "##### Exercise 9: Plot a histogram of the $p_{diffs}$. Does this plot look like what you expected? Use the matching problem to assure your understanding on what was computed here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpZqLPFO2Z1B"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "plt.hist(p_diffs)\n",
        "plt.xlabel('diff')\n",
        "plt.ylabel('Count')\n",
        "counter=0\n",
        "for i in p_diffs:\n",
        "    if i <= 0.05:\n",
        "        counter += 1\n",
        "str((counter/len(p_diffs)*100))+'%'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3ZTwlOQQLil"
      },
      "outputs": [],
      "source": [
        "# save p_diffs for later use\n",
        "# YOUR CODE HERE\n",
        "pd.DataFrame(p_diffs).to_csv('/content/p_diffs.csv', index=False)\n",
        "len(p_diffs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bA5W27K3HVl"
      },
      "source": [
        "The sample size of df2 is large enough that our sampling distribution is bell shaped. As we observed the actual difference in $(p_{new}-p_{old})$, based on the confidence interval, we have an equal difference in means between old and new pages. The normal distribution is small, that is, it's between (-0.05 and +0.05)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_BsdWfT0mxu"
      },
      "source": [
        "##### Exercise 10: What proportion of the $p_{diffs}$ are greater than the actual difference observed in ab_data.csv?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC_zA7oV2g5r"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "total = 0\n",
        "#print(pdiff.item())\n",
        "#type((p_new-p_old))\n",
        "for i in p_diffs:\n",
        "    if i > pdiff.item():\n",
        "        total += 1\n",
        "total*100/len(p_diffs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxDUjLF01CoV"
      },
      "source": [
        "##### Exercise 11: In words, explain what you just computed in Exercise 10.\n",
        "What is this value called in scientific studies?\n",
        "\n",
        "What does this value mean in terms of whether or not there is a difference between the new and old pages?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KEb5c_g1Vi4"
      },
      "outputs": [],
      "source": [
        "# What is this value called in scientific studies?\n",
        "#Answer-1 -> p-value\n",
        "#Answer-2 -> 100% p-value are less than 0.05 so null Hypothesis is true.\n",
        "# Simulate distribution under the null hypothesis\n",
        "# YOUR CODE HERE\n",
        "mu_0 = 0 #under null hypothesis mean is 0\n",
        "simulation_under_null = np.random.normal(mu_0, np.std(p_diffs), len(p_diffs))\n",
        "# Plot the null distribution\n",
        "# YOUR CODE HERE\n",
        "plt.hist(simulation_under_null)\n",
        "plt.xlabel('simulation_under_null')\n",
        "plt.ylabel('Count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V7wEymF1ba6"
      },
      "outputs": [],
      "source": [
        "# Compute p-value (consider h_alternative : p_new > p_old)\n",
        "# YOUR CODE HERE\n",
        "# n_new = 1000\n",
        "# n_old = 1000\n",
        "# treatment_size = 550\n",
        "# control_size = 500\n",
        "\n",
        "convert_new_nu = df_filtered.query(\"landing_page == 'new_page' and converted == 1\").shape[0]\n",
        "convert_old_nu = df_filtered.query(\"landing_page == 'old_page' and converted == 1\").shape[0]\n",
        "\n",
        "# Sample proportions\n",
        "p_new1 = convert_new_nu / n_new\n",
        "p_old1 = convert_old_nu / n_old\n",
        "print(treatment_size)\n",
        "print(control_size)\n",
        "print(convert_new_nu)\n",
        "print(convert_old_nu)\n",
        "# Combined proportion\n",
        "p_combined1 = (convert_new_nu+convert_old_nu)/(treatment_size + control_size)\n",
        "\n",
        "# Standard error\n",
        "SE = np.sqrt(p_combined1 * (1 - p_combined1) * (1/n_new + 1/n_old))\n",
        "print(p_combined1)\n",
        "# Test statistic\n",
        "z = (p_new1 - p_old1) / SE\n",
        "\n",
        "# P-value for one-tailed test\n",
        "p_value1 = 1 - ss.norm.cdf(z)\n",
        "\n",
        "print(f'p_new: {p_new1}')\n",
        "print(f'p_old: {p_old1}')\n",
        "print(f'Combined proportion: {p_combined1}')\n",
        "print(f'Standard error: {SE}')\n",
        "print(f'Test statistic (z): {z}')\n",
        "print(f'p-value: {p_value1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGc0JnGL1p_b"
      },
      "source": [
        "**The actual difference is captured in the population. Since $p_{value}>0.05$, we would fail to reject the null hypothesis.**\n",
        "\n",
        "100% of values from our null distribution fall to the right our actual difference. The old page has a higher probability of convertion rate than the new page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlGsRIBN2orT"
      },
      "source": [
        "##### Exercise 12: We could also use a built-in to achieve similar results. Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance.\n",
        "\n",
        "Write your code in the below cells to calculate the number of conversions for each page, as well as the number of individuals who received each page. Let $n_{old}$ and $n_{new}$ refer the the number of rows associated with the old page and new pages, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "3DdLn5Zn3HYu"
      },
      "outputs": [],
      "source": [
        "# the number of conversions for each page\n",
        "# YOUR CODE HERE\n",
        "convert_old = df_filtered.query('group == \"control\" & converted == 1').shape[0]\n",
        "convert_new = df_filtered.query('group == \"treatment\" & converted == 1').shape[0]\n",
        "# the number of individuals who received each page\n",
        "# YOUR CODE HERE\n",
        "num_old = df_filtered.shape[0] - df_filtered.query('group == \"control\"').shape[0]\n",
        "num_new = df_filtered.query('group == \"treatment\"').shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emh0ZGtI3VBv"
      },
      "source": [
        "##### Exercise 13: Now use stats.proportions_ztest to compute your test statistic and p-value. Here is a helpful [link](https://www.statsmodels.org/stable/generated/statsmodels.stats.proportion.proportions_ztest.html) on using the built in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "6F3C6J7j3cBc"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "count = np.array([convert_old, convert_new])\n",
        "nobs = np.array([num_old, num_new])\n",
        "stat, pval = proportions_ztest(count, nobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "23bgRiX03iYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265bc4e3-f1ba-4f9e-c29b-11a5147b156b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z-score '1.2863'\n",
            "p-value '0.1983'\n"
          ]
        }
      ],
      "source": [
        "# print the z_score and p_value\n",
        "# YOUR CODE HERE\n",
        "print(f\"z-score '{round(stat,4)}'\")\n",
        "print(f\"p-value '{round(pval,4)}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3FECBkC3n62"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "# Tells us how significant our z-score is?\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djj5F6on3xhV"
      },
      "outputs": [],
      "source": [
        "# Tells us what our critical value at 95% confidence is?\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fLt4UYq3_4z"
      },
      "source": [
        "##### Exercise 14: What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages? Do they agree with the findings in parts Exercise 10 and Exercise 11?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_ZfMI_4ZRd"
      },
      "source": [
        "#### **Your response here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5KSs833vhEY"
      },
      "source": [
        "**We would view various methods to conclude if $p_{new} = p_{old}$ or $p_{new} > p_{old}$:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpXGsGnwv2QT"
      },
      "source": [
        "**Method-1: Binomial Proportion Confidence Intervals**\n",
        "\n",
        "This method is quoted as the 'most common' method for A/B testing, where we find Confidence Intervals (CI) for both $p_{new}$ and $p_{old}$. If we construct similar intervals for both and compare them, we will end up in either scenario:\n",
        "\n",
        "1. The Intervals do not overlap: This implies that we can say with some level of confidence that one is better than the other, therefore providing enough evidence to reject the Null Hypothesis. This level of confidence seems to be $‚âà 1-e \\alpha^{1.91}%$ (Lan, 2011). So if there is overlap and the 95% CI are the same size, the difference is significant at the 99.5% level.\n",
        "\n",
        "2. The Intervals do overlap: Then it is either a sign that our population does not have enough statistical power, or we do not have enough evidence to reject the Null Hypothesis that $p_{new} = p_{old}$.\n",
        "\n",
        "There is a relationship between CI comparisons and hypothesis tests - given that the sample sizes are not too different and the two sets have similar standard deviations.\n",
        "\n",
        "Finding the 'true' conversion rate of a particular group is usually impossible or difficult, but we can use our calculated $p_{new}$ and $p_{old}$ as point estimations to find the Confidence Intervals for the 'true' $p_{new}$ and $p_{old}$.\n",
        "\n",
        "In this context we define $CI_{new}$ and $CI_{old}$ as below.\n",
        "* **$CI_{new}$:** The confidence interval $CI_{new}$ represents the range of values within which we estimate the true conversion rate $p_{new}$ of the new group lies with 95% confidence.\n",
        "\n",
        "* **$CI_{old}$:** The confidence interval $CI_{old}$ represents the range of values within which we estimate the true conversion rate $p_{old}$ of the old group lies with 95% confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8W59XZVA34I"
      },
      "source": [
        "Exercise 1: Define a customized function **get_z_score()** to get the z-score. Consider $\\alpha = 5$%\n",
        "\n",
        "**Hint:** You can use the inverse cumulative distribution function (CDF) of the standard normal distribution [i.e., scipy.stats.norm.ppf() in Python] to find the z-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1nRYPOTzU2e"
      },
      "outputs": [],
      "source": [
        "#function for getting z-scores for alpha. For our experiemnt where alpha = 5%, keep in mind we want to input 1-alpha/2 for Confidence Intervals.\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeYhtT7tBEDY"
      },
      "source": [
        "Exercise 2 **(Optional)**: Now calculate the $CI_{old}$ and $CI_{new}$ using the **get_z_score** and given $\\alpha = 5$%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC8GheOpybca"
      },
      "outputs": [],
      "source": [
        "# check whether Confidence Interval old (CI_old) & Confidence Interval new (CI_new) overlap?\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnsmj5hpzwdu"
      },
      "source": [
        "Both CI intervals overlap plenty as $CI_{new}$ is completely contained within $CI_{old}$ , which means we do not reject the Null Hypothesis that $p_{new} = p_{old}$.\n",
        "\n",
        "    This means that the new page is not better than the old page.\n",
        "\n",
        "While our case is quite evident that the overlap is significantly clear, slight overlaps could tempt us to draw the same conclusion to reject the Null Hypothesis. However, this is a common misinterpretation of overlapping CIs when comparing groups. Failure to do so could result in incorrect or misleading conclusions being drawn (Tan & Tan, 2010, pp. 278)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "queM7QwO0HbI"
      },
      "source": [
        "**Method-2: Z-test**\n",
        "\n",
        "We can use existing packages to calculate our test statistic and p-values and test for proportions based on the **z-test.** This is similar to the Binomial Proportion Confidence Interval Test, is quantitatively easier to draw conclusions out of due to it returning a p-value:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chYlkX-46_4x"
      },
      "source": [
        "Exercise 1: Calculate the total number of conversions for each group (convert_old & convert_new)\n",
        "\n",
        "**Hints:**\n",
        "* Use the condition (\"landing_page == 'old_page' and converted == 1\") for calculating convert_old\n",
        "* Use the condition (\"landing_page == 'new_page' and converted == 1\") for calculating convert_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sWF5r9s0e1K"
      },
      "outputs": [],
      "source": [
        "#returning the total number of conversions for each group: (convert_old & convert_new)\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rslaarjv7T6O"
      },
      "source": [
        "Exercise 2: Calculate the z_score and p_value using the one-sided z-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTimrb7E0j5s"
      },
      "outputs": [],
      "source": [
        "#calculating the z-score + p-value using the z-test (one-sided):\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzDh0qAm0u_A"
      },
      "source": [
        "Given our **p-value ‚âà 0.9 > 0.05**, we do not reject the Null Hypothesis.\n",
        "\n",
        "    This means that the new page is not better than the old page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWmxDu3m1Bm_"
      },
      "source": [
        "**Method-3: Hypothesis testing on dÃÇ and Effect Size**\n",
        "\n",
        "A couple of methdologies are found, where we also consider the pooled probability and standard deviations, under the assumption that the variances within each sample are equal. The reason we do this is so we can do a z-test under the context of our Evaluation Metric $D_{min}$, and observe if our difference is practically significant to the business:\n",
        "\n",
        "\n",
        "$\\hat{p}_{pool} =\n",
        " \\frac{X_{new} + X_{old}}{n_{new} + n_{old}}$\n",
        "\n",
        "\n",
        "$SD_{pool} =\n",
        "\\sqrt{\\hat{p}_{pool}(1 - \\hat{p}_{pool})(\\frac{1}{n_{new}} + \\frac{1}{n_{old}})}$\n",
        "\n",
        "\n",
        "All under the null hypothesis of $\\hat{d} = p_{new} - p_{old}$ where $\\hat{d} \\sim N(0,SD_{pool})$.\n",
        "\n",
        "What we will perform is the same CI calculations we've done above, but using $SD$ as our standard deviations:\n",
        "\n",
        "$CI_{diff} =\n",
        " \\hat{d} \\ \\pm SE_{pool}$\n",
        "\n",
        "After $CI_{diff}$ is calculated, the change is statistically significant if 0 lies outside the $CI_{diff}$ (equivalent to the above z-test in terms of Hypothesis Testing).\n",
        "\n",
        "However, another additional conclusion we can draw is if our Evaluation Metric $D_{min}$ is practically significant if it is outside $CI_{diff}$, especially if $D_{min}$ is below $CI_{diff}$. However, there is no statistical test that can truly tell you whether the effect is large enough to be important, so some level of subject area knowledge and expertise must be applied whether the effect is big enough to be meaningful (Frost, 2018).\n",
        "\n",
        "<u>F-test:</u>\n",
        "\n",
        "First, we must test if our variances between our treatment and control are significantly different. Therefore, we'll be using the F-test in Scipy Stats. Note that the F-Test is sensitive to non-normalities of groups, but our large enough sample size allows the Central Limit Theorem to take effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbvqB6_r-J8g"
      },
      "source": [
        "Exercise 1: Calculate the standard errors for the conversion rates of the new page (can be represented as SE_new in your code) and similarly calculate the standard errors for the conversion rates of the old page (can be represented as SE_old in your code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSbESbcA2U0L"
      },
      "outputs": [],
      "source": [
        "# Calculate the Standard Deviation or Std. Error new (SE_new)\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Calculate the Standard Deviation or Std. Error old (SE_old)\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFi_pl1f2jVK"
      },
      "source": [
        "**Using the F-test (under the Null Hypothesis that the variances are equal)** to determine if the variances within each sample are equal, noting that the **p-value = 1 - CDF** (Zach, 2020):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjACoS2e-E_N"
      },
      "source": [
        "Exercise 2: Calculate the p-value as 1 minus the Cumulative Distribution Function (CDF)\n",
        "\n",
        "**Hint:**\n",
        "You can use **[1 - scipy.stats.f.cdf()]**\n",
        "\n",
        "[Reference link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BDqdbnA24EC"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCzyJDdb2_5G"
      },
      "source": [
        "Since the p-value is almost 1, we do not reject the Null Hypothesis that the variances are the same. Now that we've verified that our variances within each sample are equal, we'll continue calculating $CI_{diff}$ :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1LaoRb9Abn4"
      },
      "source": [
        "Exercise 3:\n",
        "* Calculate the total/pooled probability of conversion (can be represented as P_pool in your code)\n",
        "* Then calculate the pooled standard deviation and\n",
        "* **Optional task:** Finally calculate the pooled Confidence Interval (can be represented as CI_diff in your code). Check whether 0 is falling within this CI_diff or not? Also check whether D_min (= 0.35%) is below the CI_diff or not? (refer to the Problem Statement for selecting D_min = 0.35%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr4UxkRv3G-9"
      },
      "outputs": [],
      "source": [
        "# Calculate the total/pooled probability of conversion:\n",
        "# YOUR CODE HERE\n",
        "\n",
        "#Calculating pooled standard deviation and pooled CI:\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0VuDkwe4hba"
      },
      "source": [
        "Because 0 falls within our $CI_{diff}$ interval, the change due to the experiment is not statistically significant, therefore we do not reject the Null Hypothesis that $\\hat{d} = p_{new} - p_{old}$ where $\\hat{d} \\sim N(0,SD_{pool})$. Furthermore, given our one-sided test where $D_{min}$ is above our confidence intervals, it is also not practically significant.\n",
        "\n",
        "**Put your remarks here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiZV_X-u60PW"
      },
      "source": [
        "**Method-4: Chi-Squared Test**\n",
        "\n",
        "One statistical test that came out is the Chi-Squared Analysis (or $\\chi^{2}$ test). If we constructed a 2x2 contingency table for our observed frequencies in our dataset, and compared it to a 2x2 contingency table for the expected frequencies in our dataset, we can perform the $\\chi^{2}$ test under the Null Hypothesis that there is no relationship that exists on between our conversion vs their treatment/control group in the population.\n",
        "\n",
        "For reference, our 2x2 contingency table will have two groups: treatment/control or converted/not converted. We want to make 4 calculations that will be in our table:\n",
        "1. Treatment, converted\n",
        "2. Treatment, not converted\n",
        "3. Control, converted\n",
        "4. Control, not converted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wajBEx9lFr4R"
      },
      "source": [
        "Exercise 1:\n",
        "* Calculate the 4 entities (treatment_converted, treatment_not_converted, control_converted & control_not_converted)\n",
        "* Create the 2x2 Contingency table which will be required to do the Chi-Square test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxf3iYzg7RrV"
      },
      "outputs": [],
      "source": [
        "# Do the 4 calculations as above:\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# create the array to do our chi-squared test: treatment/control along the rows and converted/not converted along the columns:\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHYO7N2ZFwts"
      },
      "source": [
        "Exercise 2: Carry out the Chi-Square test and estimate the p-value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M41w8pql8fJU"
      },
      "outputs": [],
      "source": [
        "# using scipy stats to perform our chi squared test:\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuczNsqz8lsr"
      },
      "source": [
        "**Give your remarks here on the results achieved from Chi-Square Test.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_59--Mj28_lu"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "The A/B testing experiment was designed to determine if <b>FaceZonGoogAppFlix</b>'s new webpage would improve the conversion rate of their users compared to their existing one.\n",
        "\n",
        "After going through multiple statistical methods to determine a winner of the A/B test, We've seen that <b>FaceZonGoogAppFlix</b>'s underlying goal had not been reached with their new webpage.\n",
        "\n",
        "Hence, we recommend that to not continue with the new webpage change, but pursue other strategies & experiments."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}