{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vchandraiitk/datascience/blob/main/M7_NB_MiniProject_2_Simple_Analytics_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hired-advertising"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: Simple Analytics using Pyspark"
      ],
      "id": "hired-advertising"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "welsh-arabic"
      },
      "source": [
        "## Problem Statement"
      ],
      "id": "welsh-arabic"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unknown-crash"
      },
      "source": [
        "Perform simple analytics with Pyspark on the real estate valuation dataset and predict the house price per unit area"
      ],
      "id": "unknown-crash"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multiple-poker"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "multiple-poker"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "congressional-statement"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* analyze the data using pyspark\n",
        "* derive the insights and visualize the data\n",
        "* implement linear regression and evaluate using pyspark"
      ],
      "id": "congressional-statement"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pleasant-tuesday"
      },
      "source": [
        "### Dataset"
      ],
      "id": "pleasant-tuesday"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reverse-circumstances"
      },
      "source": [
        "The dataset chosen for this mini-project is **Real Estate Valuation dataset**. The data was collected from the historical market of real estate within Sindian District of New Taipei City, the timespan across 2012 August to 2013 July. In the dataset, the response variable (house price per unit area) is calculated in a local unit which is approximately $10000 New Taipei Dollar per 3.3 squared meters. For the collection of regressor data, the transaction dates are transformed into a format such that 2013.250 = 2013 March, 2013.500 = 2013 June etc. The house age was collected in years and the distance to MRT stations is measured in meters.\n",
        "\n",
        "**Reference:**\n",
        "The original owner of this Real Estate Valuation dataset is professor I-Cheng Yeh from TamKang University (Department of Civil Engineering). Prof. Yeh donated this dataset to UCI machine learning repository on 18th August 2018. The dataset can be accessed at https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set#[1]."
      ],
      "id": "reverse-circumstances"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "african-police"
      },
      "source": [
        "## Grading = 10 Points"
      ],
      "id": "african-police"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voluntary-puppy"
      },
      "source": [
        "#### Install Pyspark"
      ],
      "id": "voluntary-puppy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1bcrDxx3FHC",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "f69de0b9-bc4c-4324-90d3-fc66fa4b76fe"
      },
      "source": [
        "#@title Install pyspark and Download the data\n",
        "!pip -qq install pyspark\n",
        "!pip -qq install handyspark\n",
        "!pip install numpy==1.23.0\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/RealEstate.csv\n",
        "print(\"Packages installed and dataset downloaded successfully!\")"
      ],
      "id": "-1bcrDxx3FHC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.0\n",
            "  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.0 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.0 which is incompatible.\n",
            "bigframes 1.30.0 requires numpy>=1.24.0, but you have numpy 1.23.0 which is incompatible.\n",
            "chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.0 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.23.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.0 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.0 which is incompatible.\n",
            "mizani 0.13.1 requires numpy>=1.23.5, but you have numpy 1.23.0 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.23.0 which is incompatible.\n",
            "plotnine 0.14.5 requires numpy>=1.23.5, but you have numpy 1.23.0 which is incompatible.\n",
            "pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.23.0 which is incompatible.\n",
            "scikit-image 0.25.0 requires numpy>=1.24, but you have numpy 1.23.0 which is incompatible.\n",
            "tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.0 which is incompatible.\n",
            "xarray 2025.1.0 requires numpy>=1.24, but you have numpy 1.23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "829e8468db044ac38e76adaa5d889da6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packages installed and dataset downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "protected-point"
      },
      "source": [
        "#### Import required packages"
      ],
      "id": "protected-point"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "handy-decrease"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from handyspark import *\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ],
      "id": "handy-decrease",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compatible-richardson"
      },
      "source": [
        "### Data Loading (1 point)"
      ],
      "id": "compatible-richardson"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-JqQmojhbz0"
      },
      "source": [
        "#### Start a Spark Session\n",
        "\n",
        "Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. It provides a way to interact with various spark’s functionality with a lesser number of constructs."
      ],
      "id": "1-JqQmojhbz0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focused-spring"
      },
      "source": [
        "spark = SparkSession.builder.appName('RealEstate').getOrCreate()"
      ],
      "id": "focused-spring",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZoSz4Dx4-S_"
      },
      "source": [
        "#### Load the data and infer the schema\n",
        "\n",
        "To load the dataset use the `read.csv` with `inferSchema` and `header` as parameters."
      ],
      "id": "dZoSz4Dx4-S_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "universal-minutes"
      },
      "source": [
        "csv_path = \"/content/RealEstate.csv\"\n",
        "# YOUR CODE HERE\n",
        "dataset = spark.read.csv(csv_path, inferSchema=True, header=True)"
      ],
      "id": "universal-minutes",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "described-boutique"
      },
      "source": [
        "#### Fetch the data using handyspark\n",
        "\n",
        "* Create a HandyFrame using handyspark `toHandy()` function\n",
        "\n",
        "* using an instance of `cols` from your HandyFrame, you can retrieve values for given columns in the top N rows\n",
        "\n",
        "Hint: [toHandy()](https://dvgodoy.github.io/handyspark/includeme.html)"
      ],
      "id": "described-boutique"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "handmade-concrete",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "341690d7-a106-4385-c889-219e28fd3476"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "handyFrame = dataset.toHandy()\n",
        "handyFrame.show(5)"
      ],
      "id": "handmade-concrete",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+------------+--------------------------------------+-------------------------------+-----------+------------+--------------------------+\n",
            "| No|X1 transaction date|X2 house age|X3 distance to the nearest MRT station|X4 number of convenience stores|X5 latitude|X6 longitude|Y house price of unit area|\n",
            "+---+-------------------+------------+--------------------------------------+-------------------------------+-----------+------------+--------------------------+\n",
            "|  1|           2012.917|        32.0|                              84.87882|                             10|   24.98298|   121.54024|                      37.9|\n",
            "|  2|           2012.917|        19.5|                              306.5947|                              9|   24.98034|   121.53951|                      42.2|\n",
            "|  3|           2013.583|        13.3|                              561.9845|                              5|   24.98746|   121.54391|                      47.3|\n",
            "|  4|             2013.5|        13.3|                              561.9845|                              5|   24.98746|   121.54391|                      54.8|\n",
            "|  5|           2012.833|         5.0|                              390.5684|                              5|   24.97937|   121.54245|                      43.1|\n",
            "+---+-------------------+------------+--------------------------------------+-------------------------------+-----------+------------+--------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "equivalent-anthropology"
      },
      "source": [
        "### Deriving the insights (2 points)"
      ],
      "id": "equivalent-anthropology"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spare-gather"
      },
      "source": [
        "#### show the no. of records per month\n",
        "\n",
        "Hint: Apply [groupby](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.agg.html) on transaction date and count the records using aggregation `agg()`"
      ],
      "id": "spare-gather"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spoken-nickname",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c460eb42-f6e8-4a38-dece-dfe7f35b6d98"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "dataset.groupBy(\"X1 transaction date\").agg(count(\"X1 transaction date\")).show()"
      ],
      "id": "spoken-nickname",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------------+\n",
            "|X1 transaction date|count(X1 transaction date)|\n",
            "+-------------------+--------------------------+\n",
            "|            2012.75|                        27|\n",
            "|           2013.333|                        29|\n",
            "|           2013.083|                        46|\n",
            "|           2012.833|                        31|\n",
            "|             2013.5|                        47|\n",
            "|           2013.583|                        23|\n",
            "|           2013.417|                        58|\n",
            "|           2013.167|                        25|\n",
            "|            2013.25|                        32|\n",
            "|             2013.0|                        28|\n",
            "|           2012.917|                        38|\n",
            "|           2012.667|                        30|\n",
            "+-------------------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geological-trail"
      },
      "source": [
        "#### how much is the increase in the average house price in 2012 to 2013\n",
        "\n",
        "Hint: Apply [filter](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html) on the transaction date and aggregate the house price using mean"
      ],
      "id": "geological-trail"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9Y110AJUCNM",
        "outputId": "1ff3a9be-77d2-466f-e98d-8299fcec9721"
      },
      "id": "d9Y110AJUCNM",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- X1 transaction date: double (nullable = true)\n",
            " |-- X2 house age: double (nullable = true)\n",
            " |-- X3 distance to the nearest MRT station: double (nullable = true)\n",
            " |-- X4 number of convenience stores: integer (nullable = true)\n",
            " |-- X5 latitude: double (nullable = true)\n",
            " |-- X6 longitude: double (nullable = true)\n",
            " |-- Y house price of unit area: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "educated-double",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f6fb6f-642e-4284-8879-1dfd7a3bff10"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "from pyspark.sql.types import IntegerType # Import IntegerType\n",
        "\n",
        "avg_2012 = dataset.filter(dataset[\"X1 transaction date\"].cast(IntegerType()) == 2012).agg(avg(\"Y house price of unit area\")).collect()\n",
        "avg_2013 = dataset.filter(dataset[\"X1 transaction date\"].cast(IntegerType()) == 2013).agg(avg(\"Y house price of unit area\")).collect()\n",
        "avg_2013[0][0] - avg_2012[0][0]"
      ],
      "id": "educated-double",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4084325396825506"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addressed-scratch"
      },
      "source": [
        "#### Find the count of houses with no convenience store and show the top 10 records"
      ],
      "id": "addressed-scratch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "strange-democracy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc740866-e00b-4f5b-e5e2-466e733d0ab7"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "dataset.filter(dataset[\"X4 number of convenience stores\"] == 0).count()"
      ],
      "id": "strange-democracy",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conditional-fraud"
      },
      "source": [
        "#### Compare the maximum price of houses *with convenient store* and *without convenience store*"
      ],
      "id": "conditional-fraud"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abstract-philadelphia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a2ce24-e8e9-49cb-bbd7-ae034ed2995e"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "dataset.groupBy(\"X4 number of convenience stores\").agg(max(\"Y house price of unit area\")).show()\n",
        "dataset.groupBy(\"X4 number of convenience stores\").agg(min(\"Y house price of unit area\")).show()"
      ],
      "id": "abstract-philadelphia",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------+-------------------------------+\n",
            "|X4 number of convenience stores|max(Y house price of unit area)|\n",
            "+-------------------------------+-------------------------------+\n",
            "|                              1|                          117.5|\n",
            "|                              6|                           73.6|\n",
            "|                              3|                           61.5|\n",
            "|                              5|                           60.7|\n",
            "|                              9|                           78.3|\n",
            "|                              4|                           62.9|\n",
            "|                              8|                           67.7|\n",
            "|                              7|                           62.1|\n",
            "|                             10|                           61.9|\n",
            "|                              2|                           50.5|\n",
            "|                              0|                           55.3|\n",
            "+-------------------------------+-------------------------------+\n",
            "\n",
            "+-------------------------------+-------------------------------+\n",
            "|X4 number of convenience stores|min(Y house price of unit area)|\n",
            "+-------------------------------+-------------------------------+\n",
            "|                              1|                           11.2|\n",
            "|                              6|                            7.6|\n",
            "|                              3|                           17.7|\n",
            "|                              5|                           22.8|\n",
            "|                              9|                           32.4|\n",
            "|                              4|                           21.8|\n",
            "|                              8|                           26.5|\n",
            "|                              7|                           25.0|\n",
            "|                             10|                           37.9|\n",
            "|                              2|                           20.9|\n",
            "|                              0|                           11.6|\n",
            "+-------------------------------+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "dataset.filter(col(\"X4 number of convenience stores\") > 0).agg(max(\"Y house price of unit area\").alias(\"Max with Convenience Stores\")).show()\n",
        "dataset.filter(col(\"X4 number of convenience stores\") == 0).agg(max(\"Y house price of unit area\").alias(\"Max w/o Convenience Stores\")).show()"
      ],
      "metadata": {
        "id": "s1fkZ3IRGWY6",
        "outputId": "a5367550-1d46-4880-f464-8ca3a8265b1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "s1fkZ3IRGWY6",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+\n",
            "|Max with Convenience Stores|\n",
            "+---------------------------+\n",
            "|                      117.5|\n",
            "+---------------------------+\n",
            "\n",
            "+--------------------------+\n",
            "|Max w/o Convenience Stores|\n",
            "+--------------------------+\n",
            "|                      55.3|\n",
            "+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqrJnrxt-HKE"
      },
      "source": [
        "#### Decode the transaction date\n",
        "\n",
        "* create a year column separately by removing the decimal places from transaction date column\n",
        "\n",
        "    Hint: `withColumn()`\n",
        "\n",
        "* create a month column separately based on the decimal places from the transaction date column\n",
        "\n",
        " **Hints:** multiply decimal place number with 12 and apply round off, perform below steps\n",
        "     \n",
        "     - use `udf()` from pyspark\n",
        "     - subtracting `transaction date` from `int(transaction date)` will give the decimal place number\n",
        "     - multiply the result with 12 and roundoff\n",
        "\n",
        "To know more about udf(), click [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html)"
      ],
      "id": "jqrJnrxt-HKE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dependent-restriction"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import StringType\n",
        "dataset = dataset.withColumn(\"Year\", col(\"X1 transaction date\").cast(IntegerType()))"
      ],
      "id": "dependent-restriction",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show(5)"
      ],
      "metadata": {
        "id": "ejPug14YHZeA",
        "outputId": "91dd0af6-3ed6-44e8-a456-3a30ca988a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ejPug14YHZeA",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+------------+--------------------------------------+-------------------------------+-----------+------------+--------------------------+----+\n",
            "| No|X1 transaction date|X2 house age|X3 distance to the nearest MRT station|X4 number of convenience stores|X5 latitude|X6 longitude|Y house price of unit area|Year|\n",
            "+---+-------------------+------------+--------------------------------------+-------------------------------+-----------+------------+--------------------------+----+\n",
            "|  1|           2012.917|        32.0|                              84.87882|                             10|   24.98298|   121.54024|                      37.9|2012|\n",
            "|  2|           2012.917|        19.5|                              306.5947|                              9|   24.98034|   121.53951|                      42.2|2012|\n",
            "|  3|           2013.583|        13.3|                              561.9845|                              5|   24.98746|   121.54391|                      47.3|2013|\n",
            "|  4|             2013.5|        13.3|                              561.9845|                              5|   24.98746|   121.54391|                      54.8|2013|\n",
            "|  5|           2012.833|         5.0|                              390.5684|                              5|   24.97937|   121.54245|                      43.1|2012|\n",
            "+---+-------------------+------------+--------------------------------------+-------------------------------+-----------+------------+--------------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, round, floor\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def extract_month(date_val):\n",
        "    # Use PySpark's floor and cast for Column operations\n",
        "    return round((date_val - floor(date_val)) * 12).cast(IntegerType())\n",
        "\n",
        "extract_month_udf = udf(extract_month, IntegerType())\n",
        "dataset = dataset.withColumn(\"Month\", extract_month_udf(col(\"X1 transaction date\")))\n",
        "dataset.show(5)"
      ],
      "metadata": {
        "id": "u9EolqlzH1qB",
        "outputId": "aff3ca62-ca53-4247-f29e-749831e68cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "id": "u9EolqlzH1qB",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-60-a4b50bd1d61e>\", line 6, in extract_month\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 1591, in floor\n    return _invoke_function_over_columns(\"floor\", col)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-a4b50bd1d61e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mextract_month_udf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_month\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Month\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_month_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X1 transaction date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-60-a4b50bd1d61e>\", line 6, in extract_month\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 1591, in floor\n    return _invoke_function_over_columns(\"floor\", col)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t52ChFvtJBvb",
        "outputId": "de598588-46c1-44ba-b129-5930e2c65387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "id": "t52ChFvtJBvb",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-57-3d4ec2c77a85>\", line 5, in extract_month\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 1591, in floor\n    return _invoke_function_over_columns(\"floor\", col)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-cc0491a82a7c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-57-3d4ec2c77a85>\", line 5, in extract_month\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 1591, in floor\n    return _invoke_function_over_columns(\"floor\", col)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "def extract_month(date_val):\n",
        "    return round((date_val - int(date_val)) * 12)\n",
        "\n",
        "extract_month_udf = udf(extract_month, IntegerType())\n",
        "dataset = dataset.withColumn(\"Month\", extract_month_udf(col(\"X1 transaction date\")))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jASGUCj4K8Gr"
      },
      "id": "jASGUCj4K8Gr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "established-television"
      },
      "source": [
        "### Data Visualization (2 points)"
      ],
      "id": "established-television"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "natural-hughes"
      },
      "source": [
        "#### Select the continuous variables from the data and visualize using histogram"
      ],
      "id": "natural-hughes"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "present-challenge"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "present-challenge",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "developed-remedy"
      },
      "source": [
        "#### Visualize the transaction date using the countplot"
      ],
      "id": "developed-remedy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "protected-science"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "protected-science",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "normal-creek"
      },
      "source": [
        "#### visualize number of convenience stores with appropriate plot."
      ],
      "id": "normal-creek"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "harmful-parameter"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "harmful-parameter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geographic-praise"
      },
      "source": [
        "#### visualize the geographical distribution of the house prices of unit area\n",
        "\n",
        "With:\n",
        "\n",
        "* x-axis = X6 longitude\n",
        "* y-axis = X5 latitude\n",
        "* datapoints = Y house price of unit area\n",
        "* parameters including size `s` and color map `cmap`"
      ],
      "id": "geographic-praise"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "varying-motel"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "varying-motel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sharp-alert"
      },
      "source": [
        "### Feature Scaling (1 point)"
      ],
      "id": "sharp-alert"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "right-anthropology"
      },
      "source": [
        "#### Identify the outliers\n",
        "\n",
        "Use the pairplot or boxplot to identify the outliers\n",
        "\n",
        "   **Hint:** `sns.pairplot`"
      ],
      "id": "right-anthropology"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exceptional-promotion"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "exceptional-promotion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accessible-absolute"
      },
      "source": [
        "#### Correlation analysis\n",
        "\n",
        "create the correlation matrix of all the columns and visualize using the heatmap"
      ],
      "id": "accessible-absolute"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arbitrary-albania"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "arbitrary-albania",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experimental-breeding"
      },
      "source": [
        "#### Normalization or standardization\n",
        "\n",
        "select the applicable features using vector assembler and apply scaling using `MinMaxScaler` from pyspark\n",
        "\n",
        "Hint: [MinMaxScaler](https://spark.apache.org/docs/latest/ml-features#minmaxscaler)"
      ],
      "id": "experimental-breeding"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enabling-rwanda"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "enabling-rwanda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "looking-ordinance"
      },
      "source": [
        "### Feature Engineering (2 points)"
      ],
      "id": "looking-ordinance"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "several-popularity"
      },
      "source": [
        "#### Assemble the features\n",
        "\n",
        "concatenate all the features into a single vector which can be further passed to the ML algorithm.\n",
        "\n",
        "**Hint:** `VectorAssembler()`"
      ],
      "id": "several-popularity"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shaped-transaction"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "shaped-transaction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extra-negative"
      },
      "source": [
        "#### Feature selection and splitting the data"
      ],
      "id": "extra-negative"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occupied-depth"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "occupied-depth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "varied-progressive"
      },
      "source": [
        "### Train and Evaluate the model (2 points)"
      ],
      "id": "varied-progressive"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "motivated-compiler"
      },
      "source": [
        "Using LinearRegression from `pyspark.ml` fit the data and find the coefficients and intercept"
      ],
      "id": "motivated-compiler"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "close-compact"
      },
      "source": [
        "regressor = LinearRegression(featuresCol='features', labelCol='label')"
      ],
      "id": "close-compact",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advised-forth"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "advised-forth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pressing-sitting"
      },
      "source": [
        "Get the predictions and show the table along with actual data and predicted data"
      ],
      "id": "pressing-sitting"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "minute-encoding"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "minute-encoding",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "living-mistake"
      },
      "source": [
        "Calculate the RMSE and $R^2$ of the model"
      ],
      "id": "living-mistake"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "metric-bench"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "metric-bench",
      "execution_count": null,
      "outputs": []
    }
  ]
}